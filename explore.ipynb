{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e2174c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--root ROOT] [--out OUT]\n",
      "                             [--workers WORKERS]\n",
      "                             [--max-seq-depth MAX_SEQ_DEPTH]\n",
      "                             [--include-private] [--keep-pixeldata]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/run/user/1974645248/jupyter/runtime/kernel-v3cfc045a8cfd50a2bbd6353478e0ded8e5b70f98a.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eo287/miniconda/envs/eo_ccta/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Parallel inventory of CCTA DICOMs by accession (E100..., E101..., etc.), one row per SeriesInstanceUID.\n",
    "- Fast pass over all files using specific_tags to group + aggregate without pixel data.\n",
    "- Full header read only once per series (representative file) to export *all* header fields.\n",
    "- Parallelized over accession folders.\n",
    "\n",
    "Usage:\n",
    "    python inventory_ccta_full_parallel.py \\\n",
    "        --root /home/eo287/mnt/s3_ccta/cta_09232025/studies \\\n",
    "        --out  /home/eo287/mnt/s3_ccta/summaries/ccta_series_inventory_fullheaders.csv \\\n",
    "        --workers 8\n",
    "\n",
    "Env:\n",
    "    python>=3.11, pydicom, pandas, tqdm\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import argparse\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "from collections import defaultdict, Counter\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pydicom\n",
    "from pydicom.tag import BaseTag\n",
    "from pydicom.datadict import keyword_for_tag, dictionary_description\n",
    "from pydicom.errors import InvalidDicomError\n",
    "\n",
    "# ----------------- Defaults / Tunables -----------------\n",
    "DEFAULT_ROOT = \"/home/eo287/mnt/s3_ccta/cta_09232025/studies\"\n",
    "DEFAULT_OUT = \"/home/eo287/mnt/s3_ccta/summaries/ccta_series_inventory_fullheaders.csv\"\n",
    "\n",
    "# Flattening controls\n",
    "FLATTEN_SEQUENCES: bool = True\n",
    "MAX_SEQ_DEPTH: int = 2         # flatten nested sequences up to this depth\n",
    "MAX_STRING_LEN: int = 2000     # truncate very long string-ish values for CSV hygiene\n",
    "INCLUDE_PRIVATE: bool = True   # keep private tags\n",
    "EXCLUDE_PIXELDATA: bool = True # drop PixelData and very large binary blobs\n",
    "\n",
    "# Fast-pass tags (IO-efficient; used to group/aggregate without loading full headers):\n",
    "FAST_TAGS = [\"SeriesInstanceUID\", \"InstanceNumber\", \"ImagePositionPatient\", \"AcquisitionTime\"]\n",
    "\n",
    "# ----------------- Helpers -----------------\n",
    "def is_dicom_path(path: str) -> bool:\n",
    "    return path.lower().endswith(\".dcm\")\n",
    "\n",
    "def safe_read_header_full(path: str) -> Optional[pydicom.dataset.FileDataset]:\n",
    "    \"\"\"Full header (top-level) without pixel data; used once per series.\"\"\"\n",
    "    try:\n",
    "        return pydicom.dcmread(path, stop_before_pixels=True, force=True)\n",
    "    except (InvalidDicomError, Exception):\n",
    "        return None\n",
    "\n",
    "def safe_read_header_fast(path: str) -> Optional[pydicom.dataset.FileDataset]:\n",
    "    \"\"\"Fast, partial header for grouping/aggregates (very small tag set).\"\"\"\n",
    "    try:\n",
    "        return pydicom.dcmread(path, stop_before_pixels=True, force=True, specific_tags=FAST_TAGS)\n",
    "    except (InvalidDicomError, Exception):\n",
    "        return None\n",
    "\n",
    "def tag_to_key(tag: BaseTag) -> str:\n",
    "    \"\"\"Prefer keyword; else human name; else Private_(ggggeeee).\"\"\"\n",
    "    kw = keyword_for_tag(tag)\n",
    "    if kw:\n",
    "        return kw\n",
    "    name = dictionary_description(tag)\n",
    "    if name:\n",
    "        return name.replace(\" \", \"\")\n",
    "    return f\"Private_({int(tag):08X})\"\n",
    "\n",
    "def _truncate(v: str) -> str:\n",
    "    if v is None:\n",
    "        return \"NA\"\n",
    "    if isinstance(v, str) and len(v) > MAX_STRING_LEN:\n",
    "        return v[:MAX_STRING_LEN] + \"...<truncated>\"\n",
    "    return str(v)\n",
    "\n",
    "def element_to_value(elem: pydicom.dataelem.DataElement, depth: int = 0) -> Any:\n",
    "    \"\"\"Serialize a DataElement value; flatten sequences if requested.\"\"\"\n",
    "    if EXCLUDE_PIXELDATA and elem.keyword == \"PixelData\":\n",
    "        return \"<PixelData omitted>\"\n",
    "    val = elem.value\n",
    "    # Sequence flattening\n",
    "    if elem.VR == \"SQ\" and FLATTEN_SEQUENCES:\n",
    "        if depth >= MAX_SEQ_DEPTH:\n",
    "            try:\n",
    "                return _truncate(json.dumps([f\"Item{idx}\" for idx, _ in enumerate(val)], ensure_ascii=False))\n",
    "            except Exception:\n",
    "                return f\"<SQ depth>{len(val)} items\"\n",
    "        items: List[Dict[str, Any]] = []\n",
    "        for it in val:\n",
    "            items.append(dataset_to_dict(it, depth=depth + 1))\n",
    "        try:\n",
    "            return _truncate(json.dumps(items, ensure_ascii=False))\n",
    "        except Exception:\n",
    "            return _truncate(str(items))\n",
    "    # Multi-values normalize\n",
    "    if isinstance(val, (list, tuple)):\n",
    "        return _truncate(\"|\".join(_truncate(str(x)) for x in val))\n",
    "    # Numeric NaNs to \"NA\"\n",
    "    if isinstance(val, float) and (math.isnan(val) or math.isinf(val)):\n",
    "        return \"NA\"\n",
    "    return _truncate(str(val))\n",
    "\n",
    "def dataset_to_dict(ds: pydicom.dataset.Dataset, depth: int = 0) -> Dict[str, Any]:\n",
    "    out: Dict[str, Any] = {}\n",
    "    for elem in ds:\n",
    "        if not INCLUDE_PRIVATE and elem.tag.is_private:\n",
    "            continue\n",
    "        key = elem.keyword or tag_to_key(elem.tag)\n",
    "        try:\n",
    "            out[key] = element_to_value(elem, depth=depth)\n",
    "        except Exception:\n",
    "            out[key] = \"<serialization_error>\"\n",
    "    return out\n",
    "\n",
    "# ----------------- Accession processing -----------------\n",
    "def process_accession(acc_root: str, acc_name: str) -> Tuple[List[Dict[str, Any]], set]:\n",
    "    \"\"\"\n",
    "    Process a single accession (runs in a worker).\n",
    "    Returns list of rows (dicts) and the union of keys observed.\n",
    "    \"\"\"\n",
    "    acc_path = os.path.join(acc_root, acc_name)\n",
    "\n",
    "    # First pass: map files to series and collect aggregates without full reads\n",
    "    # series_map: siuid -> list of filepaths\n",
    "    series_map: Dict[str, List[str]] = defaultdict(list)\n",
    "    # aggregates per series we can compute on the fly:\n",
    "    inst_nums: Dict[str, List[int]] = defaultdict(list)\n",
    "    z_coords: Dict[str, List[float]] = defaultdict(list)\n",
    "    acq_times: Dict[str, List[str]] = defaultdict(list)\n",
    "    read_errors: Dict[str, int] = defaultdict(int)\n",
    "\n",
    "    # For picking representative later:\n",
    "    # maintain (min_instance_number, filepath) for each series\n",
    "    rep_candidate: Dict[str, Tuple[int, str]] = {}\n",
    "\n",
    "    for dirpath, _, files in os.walk(acc_path):\n",
    "        for fn in files:\n",
    "            if not is_dicom_path(fn):\n",
    "                continue\n",
    "            fpath = os.path.join(dirpath, fn)\n",
    "            ds = safe_read_header_fast(fpath)\n",
    "            if ds is None:\n",
    "                # can't even find series; skip\n",
    "                continue\n",
    "            siuid = getattr(ds, \"SeriesInstanceUID\", None)\n",
    "            if not siuid:\n",
    "                continue\n",
    "            siuid = str(siuid)\n",
    "            series_map[siuid].append(fpath)\n",
    "\n",
    "            # aggregates\n",
    "            inum = getattr(ds, \"InstanceNumber\", None)\n",
    "            if inum is not None:\n",
    "                try:\n",
    "                    inum_int = int(inum)\n",
    "                    inst_nums[siuid].append(inum_int)\n",
    "                    # candidate rep: smallest instance number\n",
    "                    prev = rep_candidate.get(siuid)\n",
    "                    if prev is None or inum_int < prev[0]:\n",
    "                        rep_candidate[siuid] = (inum_int, fpath)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            else:\n",
    "                # no instance number: prefer first encountered if no candidate\n",
    "                if siuid not in rep_candidate:\n",
    "                    rep_candidate[siuid] = (10**9, fpath)  # large placeholder\n",
    "\n",
    "            ipp = getattr(ds, \"ImagePositionPatient\", None)\n",
    "            if ipp and isinstance(ipp, (list, tuple)) and len(ipp) == 3:\n",
    "                try:\n",
    "                    z_coords[siuid].append(float(ipp[2]))\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            at = getattr(ds, \"AcquisitionTime\", None)\n",
    "            if at:\n",
    "                acq_times[siuid].append(str(at))\n",
    "\n",
    "    # Build rows: for each series, read *full header once* from representative\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    union_keys: set = set()\n",
    "\n",
    "    for siuid, paths in series_map.items():\n",
    "        # Representative\n",
    "        rep_path = rep_candidate.get(siuid, (None, None))[1] or paths[0]\n",
    "        ds_full = safe_read_header_full(rep_path)\n",
    "        if ds_full is None:\n",
    "            # try to find any other readable file as representative\n",
    "            for candidate in paths:\n",
    "                ds_full = safe_read_header_full(candidate)\n",
    "                if ds_full is not None:\n",
    "                    rep_path = candidate\n",
    "                    break\n",
    "        if ds_full is None:\n",
    "            # skip this series entirely\n",
    "            continue\n",
    "\n",
    "        # header dict (flattened)\n",
    "        header_dict = dataset_to_dict(ds_full, depth=0)\n",
    "\n",
    "        # aggregates\n",
    "        n_images = len(paths)\n",
    "        inst_min = min(inst_nums[siuid]) if inst_nums.get(siuid) else \"NA\"\n",
    "        inst_max = max(inst_nums[siuid]) if inst_nums.get(siuid) else \"NA\"\n",
    "        if z_coords.get(siuid):\n",
    "            zspan = max(z_coords[siuid]) - min(z_coords[siuid])\n",
    "        else:\n",
    "            zspan = \"NA\"\n",
    "        acq_mode = Counter(acq_times.get(siuid, [])).most_common(1)[0][0] if acq_times.get(siuid) else \"NA\"\n",
    "\n",
    "        row: Dict[str, Any] = {\n",
    "            \"AccessionFolder\": acc_name,\n",
    "            \"SeriesInstanceUID\": siuid,\n",
    "            \"NumImages\": n_images,\n",
    "            \"InstanceNumberMin\": inst_min,\n",
    "            \"InstanceNumberMax\": inst_max,\n",
    "            \"ZSpan\": zspan,\n",
    "            \"AcquisitionTimeMode\": acq_mode,\n",
    "            \"NumHeaderReadErrors\": 0,  # retained for symmetry; we skip unreadables earlier\n",
    "            \"RepresentativeFile\": rep_path,\n",
    "        }\n",
    "        row.update(header_dict)\n",
    "        rows.append(row)\n",
    "        union_keys.update(row.keys())\n",
    "\n",
    "    return rows, union_keys\n",
    "\n",
    "# ----------------- Main -----------------\n",
    "def main():\n",
    "    global MAX_SEQ_DEPTH, INCLUDE_PRIVATE, EXCLUDE_PIXELDATA\n",
    "\n",
    "    ap = argparse.ArgumentParser(description=\"Parallel CCTA DICOM series inventory (full headers).\")\n",
    "    ap.add_argument(\"--root\", default=DEFAULT_ROOT,\n",
    "                    help=\"Root folder containing accession folders (E100..., E101...)\")\n",
    "    ap.add_argument(\"--out\",  default=DEFAULT_OUT,\n",
    "                    help=\"Output CSV path\")\n",
    "    ap.add_argument(\"--workers\", type=int, default=min(8, cpu_count()),\n",
    "                    help=\"Number of parallel worker processes (default=min(8, cpu_count()))\")\n",
    "    # safe to reference MAX_SEQ_DEPTH now that global is declared\n",
    "    ap.add_argument(\"--max-seq-depth\", type=int, default=MAX_SEQ_DEPTH,\n",
    "                    help=\"Max sequence flattening depth\")\n",
    "    ap.add_argument(\"--include-private\", action=\"store_true\", default=INCLUDE_PRIVATE,\n",
    "                    help=\"Include private tags\")\n",
    "    ap.add_argument(\"--keep-pixeldata\", action=\"store_true\",\n",
    "                    help=\"Include PixelData (not recommended)\")\n",
    "\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    # Update globals from CLI\n",
    "    MAX_SEQ_DEPTH   = args.max_seq_depth\n",
    "    INCLUDE_PRIVATE = args.include_private\n",
    "    EXCLUDE_PIXELDATA = not args.keep_pixeldata\n",
    "\n",
    "    root = args.root\n",
    "    out_csv = args.out\n",
    "\n",
    "    if not os.path.isdir(root):\n",
    "        print(f\"ERROR: root not found: {root}\")\n",
    "        sys.exit(2)\n",
    "\n",
    "    # Discover accession folders\n",
    "    accessions = sorted(\n",
    "        d for d in os.listdir(root)\n",
    "        if os.path.isdir(os.path.join(root, d)) and d[0].upper() == \"E\"\n",
    "    )\n",
    "    if not accessions:\n",
    "        print(\"No accession folders (E...) found under:\", root)\n",
    "        sys.exit(0)\n",
    "\n",
    "    all_rows: List[Dict[str, Any]] = []\n",
    "    union_keys: set = set()\n",
    "\n",
    "    # Parallel map over accessions\n",
    "    with ProcessPoolExecutor(max_workers=args.workers) as ex:\n",
    "        futures = {ex.submit(process_accession, root, acc): acc for acc in accessions}\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), desc=\"Accessions (parallel)\"):\n",
    "            acc = futures[fut]\n",
    "            try:\n",
    "                rows, keys = fut.result()\n",
    "                if rows:\n",
    "                    all_rows.extend(rows)\n",
    "                    union_keys.update(keys)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Accession {acc} failed: {e}\", file=sys.stderr)\n",
    "\n",
    "    if not all_rows:\n",
    "        print(\"No series discovered; nothing to write.\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    # Column ordering\n",
    "    base_cols = [\n",
    "        \"AccessionFolder\", \"SeriesInstanceUID\",\n",
    "        \"NumImages\", \"InstanceNumberMin\", \"InstanceNumberMax\", \"ZSpan\",\n",
    "        \"AcquisitionTimeMode\", \"NumHeaderReadErrors\", \"RepresentativeFile\",\n",
    "    ]\n",
    "    likely_keys = [\n",
    "        \"AccessionNumber\", \"StudyInstanceUID\", \"StudyID\", \"StudyDate\", \"StudyTime\", \"StudyDescription\",\n",
    "        \"PatientID\", \"PatientSex\", \"PatientBirthDate\",\n",
    "        \"SeriesNumber\", \"SeriesDescription\", \"Modality\",\n",
    "        \"Manufacturer\", \"ManufacturerModelName\", \"InstitutionName\",\n",
    "    ]\n",
    "    columns = list(base_cols)\n",
    "    for k in likely_keys:\n",
    "        if k in union_keys and k not in columns:\n",
    "            columns.append(k)\n",
    "    for k in sorted(union_keys):\n",
    "        if k not in columns:\n",
    "            columns.append(k)\n",
    "\n",
    "    df = pd.DataFrame(all_rows, columns=columns)\n",
    "\n",
    "    sort_keys = [c for c in [\"AccessionFolder\", \"StudyDate\", \"SeriesNumber\"] if c in df.columns]\n",
    "    if sort_keys:\n",
    "        df = df.sort_values(sort_keys, kind=\"stable\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_csv), exist_ok=True)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "\n",
    "    print(f\"Wrote {len(df):,} series rows with {len(df.columns):,} columns to:\\n  {out_csv}\")\n",
    "    with pd.option_context(\"display.max_columns\", 20, \"display.width\", 220):\n",
    "        print(df.head(5))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.argv = [\"inventory_ccta_full_parallel.py\"]\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eo_ccta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
